{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from pandas.tseries.offsets import BDay\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##top stocks \n",
    "stockPairList = [('AMZN','CRM'), ('MSFT', 'EBAY'), ('NVDA', 'TSM') ]\n",
    "stockPairNames = [('Amazon', 'Salesforce'), ('Microsoft', 'Ebay'), ('NVIDIA', 'TSMC')]\n",
    "url = 'https://newsapi.org/v2/everything?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trade_open(date):\n",
    "    curr_date_open = pd.to_datetime(date).floor('d').replace(hour=13,minute=30) - BDay(0)\n",
    "    curr_date_close = pd.to_datetime(date).floor('d').replace(hour=20,minute=0) - BDay(0)\n",
    "    \n",
    "    prev_date_close = (curr_date_open - BDay()).replace(hour=20,minute=0)\n",
    "    next_date_open = (curr_date_close + BDay()).replace(hour=13,minute=30)\n",
    "    \n",
    "    if ((pd.to_datetime(date)>=prev_date_close) & (pd.to_datetime(date)<curr_date_open)):\n",
    "        return curr_date_open\n",
    "    elif ((pd.to_datetime(date)>=curr_date_close) & (pd.to_datetime(date)<next_date_open)):\n",
    "        return next_date_open\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStockSentiment(stock):\n",
    "    parameters = {\n",
    "        'q': stock, # query phrase\n",
    "        'sortBy': 'popularity', # articles from popular sources and publishers come first\n",
    "        'pageSize': 100,  # maximum is 100 for developer version\n",
    "        'apiKey': '4a123522d48a4ed397f18a0b760dc4fc', # your own API key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=parameters)\n",
    "    data = pd.DataFrame(response.json())\n",
    "\n",
    "    news_df = pd.concat([data['articles'].apply(pd.Series)], axis=1)\n",
    "\n",
    "    final_news = news_df.loc[:,['publishedAt','title']]\n",
    "    final_news['publishedAt'] = pd.to_datetime(final_news['publishedAt'])\n",
    "    final_news.sort_values(by='publishedAt',inplace=True)\n",
    "    final_news[\"trading_time\"] = final_news[\"publishedAt\"].apply(get_trade_open)\n",
    "    final_news = final_news[pd.notnull(final_news['trading_time'])]\n",
    "    final_news['Date'] = pd.to_datetime(pd.to_datetime(final_news['trading_time']).dt.date)\n",
    "    cs = []\n",
    "    for row in range(len(final_news)):\n",
    "        cs.append(analyzer.polarity_scores(final_news['title'].iloc[row])['compound'])\n",
    "\n",
    "    final_news['compound_vader_score'] = cs\n",
    "    final_news = final_news[(final_news[['compound_vader_score']] != 0).all(axis=1)].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    unique_dates = final_news['Date'].unique()\n",
    "    grouped_dates = final_news.groupby(['Date'])\n",
    "    keys_dates = list(grouped_dates.groups.keys())\n",
    "\n",
    "    max_cs = []\n",
    "    min_cs = []\n",
    "\n",
    "    for key in grouped_dates.groups.keys():\n",
    "        data = grouped_dates.get_group(key)\n",
    "        if data[\"compound_vader_score\"].max() > 0:\n",
    "            max_cs.append(data[\"compound_vader_score\"].max())\n",
    "        elif data[\"compound_vader_score\"].max() < 0:\n",
    "            max_cs.append(0)\n",
    "\n",
    "        if data[\"compound_vader_score\"].min() < 0:\n",
    "            min_cs.append(data[\"compound_vader_score\"].min())\n",
    "        elif data[\"compound_vader_score\"].min() > 0:\n",
    "            min_cs.append(0)\n",
    "\n",
    "    extreme_scores_dict = {'Date':keys_dates,'max_scores':max_cs,'min_scores':min_cs}\n",
    "    extreme_scores_df = pd.DataFrame(extreme_scores_dict)\n",
    "    \n",
    "    final_scores = []\n",
    "    for i in range(len(extreme_scores_df)):\n",
    "        final_scores.append(extreme_scores_df['max_scores'].values[i] + extreme_scores_df['min_scores'].values[i])\n",
    "\n",
    "    extreme_scores_df['final_scores'] = final_scores\n",
    "\n",
    "    return extreme_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nandhitharaghuram/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/Users/nandhitharaghuram/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/Users/nandhitharaghuram/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "returns = []\n",
    "for pair in stockPairList:\n",
    "    stock1 = pair[0]\n",
    "    stock2 = pair[1]\n",
    "    df = pd.read_csv('SentimentStocks/'+stock1+'_'+stock2+'.csv')\n",
    "    df = df.drop('Unnamed: 0',1)\n",
    "    df['Date']= pd.to_datetime(df['Date'])\n",
    "    stock1Sentiment = getStockSentiment(stock1)\n",
    "    stock2Sentiment = getStockSentiment(stock2)\n",
    "    #merge df stock1Sentiment stock2Sentiment\n",
    "    mergedSentimentDf = stock1Sentiment.merge(stock2Sentiment, how='inner', on='Date')\n",
    "    mergedDf = df.merge(mergedSentimentDf, how = 'inner', on='Date')\n",
    "    mergedDf = mergedDf.drop('returns',1)\n",
    "    mergedDf['scores'] = mergedDf['final_scores_x'] + mergedDf['final_scores_y'] \n",
    "    for i in range (mergedDf.shape[0]):\n",
    "            if mergedDf['z_score'].iloc[i] + mergedDf['scores'].iloc[i] < -1:\n",
    "                mergedDf['position_1'].iloc[i] = 1\n",
    "                mergedDf['position_2'].iloc[i] = -round(mergedDf['ratio'].iloc[i],0)\n",
    "            if mergedDf['z_score'].iloc[i] + mergedDf['scores'].iloc[i] > 1:\n",
    "                mergedDf['position_1'].iloc[i] = -1\n",
    "                mergedDf['position_2'].iloc[i] = round(mergedDf['ratio'].iloc[i],0)\n",
    "            if (abs(mergedDf['z_score'].iloc[i] + mergedDf['scores'].iloc[i]) < 1) & (abs(mergedDf['z_score'].iloc[i] + mergedDf['scores'].iloc[i]) > 0):\n",
    "                mergedDf['position_1'].iloc[i] = 0\n",
    "                mergedDf['position_2'].iloc[i] = 0\n",
    "    mergedDf['returns'] = ((mergedDf[stock1]-mergedDf[stock1].shift(1))/mergedDf[stock1].shift(1))*mergedDf['position_1'].shift(1)+ ((mergedDf[stock2]-mergedDf[stock2].shift(1))/mergedDf[stock2].shift(1))*mergedDf['position_2'].shift(1)\n",
    "    mergedDf.to_csv(\"SentimentStocks/Results/\"+stock1+\"_\"+stock2+\".csv\")\n",
    "    mergedDf['returns'].sum()\n",
    "    returns.append(mergedDf['returns'].sum())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.5481273470820176, 0.1218583390540198, -0.0003726051287071541]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
